"""
Adversarial IoT Environment for Blue Team Training.

This module implements the Gymnasium environment where the RL agent
(Blue Team) learns to defend against attacks generated by the Attack
Sequence Generator (Red Team).

Key Design (PRD Section 5):
- Hidden State: Attack stage is not directly observable
- Box Observation Space: Window of realized feature vectors
- Discrete Action Space: 5 force continuum levels
- Reward: R_t = R_defense - C_action - P_impact

The environment bridges the Attack Sequence Generator (abstract states)
with realistic network traffic features via the Realization Engine.
"""

import logging
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union

import gymnasium as gym
import numpy as np
import torch
from gymnasium import spaces

from src.generator.attack_sequence_generator import AttackSequenceGenerator
from src.utils.label_mapper import KillChainStage
from src.utils.realization_engine import RealizationEngine

logger = logging.getLogger(__name__)

# =============================================================================
# ACTION SPACE: Force Continuum
# =============================================================================

ACTION_NAMES: List[str] = [
    "OBSERVE",   # Level 0: Passive monitoring
    "LOG",       # Level 1: Enhanced logging
    "THROTTLE",  # Level 2: Rate limiting
    "BLOCK",     # Level 3: Block connections
    "ISOLATE",   # Level 4: System isolation
]

ACTION_COSTS: List[float] = [
    0.0,   # OBSERVE: No cost
    0.1,   # LOG: Minimal impact
    0.3,   # THROTTLE: Some disruption
    0.5,   # BLOCK: Significant impact
    0.8,   # ISOLATE: Major disruption
]


def get_action_cost(action: int) -> float:
    """Get the cost of a defensive action.
    
    Args:
        action: Action index (0-4).
    
    Returns:
        Cost of the action.
    """
    return ACTION_COSTS[action]


def get_action_name(action: int) -> str:
    """Get the name of a defensive action.
    
    Args:
        action: Action index (0-4).
    
    Returns:
        Name of the action.
    """
    return ACTION_NAMES[action]


# =============================================================================
# ENVIRONMENT CONFIGURATION
# =============================================================================

@dataclass
class AdversarialEnvConfig:
    """Configuration for the Adversarial Environment.
    
    Attributes:
        max_steps: Maximum steps per episode.
        window_size: Number of observations in sliding window.
        num_actions: Number of discrete actions (force continuum levels).
        action_cost_scale: Multiplier for action costs.
        impact_penalty: Penalty when attack reaches IMPACT stage.
        defense_success_bonus: Bonus for successful defense.
        correct_escalation_reward: Reward for escalating defense appropriately.
        correct_de_escalation_reward: Reward for de-escalating appropriately.
        maintained_defense_reward: Reward for holding appropriate level.
    """
    
    max_steps: int = 500
    window_size: int = 5
    num_actions: int = 5
    action_cost_scale: float = 1.0
    impact_penalty: float = 5.0
    defense_success_bonus: float = 2.0
    correct_escalation_reward: float = 1.0
    correct_de_escalation_reward: float = 0.5
    maintained_defense_reward: float = 0.2


# =============================================================================
# ADVERSARIAL ENVIRONMENT
# =============================================================================

class AdversarialIoTEnv(gym.Env):
    """Gymnasium environment for adversarial IoT defense training.
    
    This environment implements a partially observable Markov decision
    process where the agent observes realized network features but
    cannot directly see the underlying attack stage.
    
    Red Team (Attack Sequence Generator):
        - Controls attack progression through Kill Chain stages
        - Uses LSTM to generate realistic attack sequences
    
    Blue Team (RL Agent):
        - Observes window of feature vectors
        - Takes discrete defensive actions
        - Receives reward based on defense effectiveness
    
    Observation Space:
        Box of shape (window_size * num_features,)
        
    Action Space:
        Discrete(5): OBSERVE, LOG, THROTTLE, BLOCK, ISOLATE
    
    Example:
        >>> env = AdversarialIoTEnv(generator_path, dataset_path)
        >>> obs, info = env.reset()
        >>> obs, reward, terminated, truncated, info = env.step(2)
    """
    
    metadata = {"render_modes": ["human"]}
    
    def __init__(
        self,
        generator_path: Union[str, Path],
        dataset_path: Union[str, Path],
        config: Optional[AdversarialEnvConfig] = None,
        render_mode: Optional[str] = None,
        device: str = "cpu",
    ) -> None:
        """Initialize the Adversarial Environment.
        
        Args:
            generator_path: Path to trained Attack Sequence Generator.
            dataset_path: Path to processed dataset for realization.
            config: Environment configuration.
            render_mode: Rendering mode (optional).
            device: Device for generator inference.
        """
        super().__init__()
        
        self._config = config or AdversarialEnvConfig()
        self._render_mode = render_mode
        self._device = torch.device(device)
        
        # Load Attack Sequence Generator (Red Team)
        generator_path = Path(generator_path)
        model_file = generator_path / "attack_sequence_generator.pth"
        self._generator = AttackSequenceGenerator.load(model_file, device=self._device)
        self._generator.eval()
        
        # Load Realization Engine (for feature sampling)
        dataset_path = Path(dataset_path)
        self._realization_engine = RealizationEngine(dataset_path)
        
        # Get feature dimension from dataset
        self._num_features = self._realization_engine.num_features
        
        # Define observation space: flattened window of features
        obs_dim = self._config.window_size * self._num_features
        self.observation_space = spaces.Box(
            low=-np.inf,
            high=np.inf,
            shape=(obs_dim,),
            dtype=np.float32,
        )
        
        # Define action space: force continuum levels
        self.action_space = spaces.Discrete(self._config.num_actions)
        
        # Episode state (initialized in reset)
        self._step_count: int = 0
        self._current_attack_stage: int = 0
        self._attack_history: List[int] = []
        self._observation_window: List[np.ndarray] = []
        self._last_action: int = 0
        self._rng: Optional[np.random.Generator] = None
        
        logger.info(
            f"AdversarialIoTEnv initialized: "
            f"obs_shape={self.observation_space.shape}, "
            f"actions={self._config.num_actions}, "
            f"features={self._num_features}"
        )
    
    # =========================================================================
    # Gymnasium API
    # =========================================================================
    
    def reset(
        self,
        *,
        seed: Optional[int] = None,
        options: Optional[Dict[str, Any]] = None,
    ) -> Tuple[np.ndarray, Dict[str, Any]]:
        """Reset the environment to start a new episode.
        
        Args:
            seed: Random seed for reproducibility.
            options: Additional options (unused).
        
        Returns:
            Tuple of (observation, info).
        """
        super().reset(seed=seed)
        
        # Setup RNG
        if seed is not None:
            self._rng = np.random.default_rng(seed)
            # Also seed the realization engine
            self._realization_engine._rng = np.random.default_rng(seed)
            # And set torch seed for generator
            torch.manual_seed(seed)
            np.random.seed(seed)
        else:
            self._rng = np.random.default_rng()
        
        # Reset episode state
        self._step_count = 0
        self._last_action = 0
        
        # Initialize attack sequence
        # Start with BENIGN or low-level attack
        self._current_attack_stage = 0  # BENIGN
        self._attack_history = [self._current_attack_stage]
        
        # Initialize observation window with BENIGN features
        self._observation_window = []
        for _ in range(self._config.window_size):
            features = self._realization_engine.sample(KillChainStage.BENIGN)
            self._observation_window.append(features)
        
        # Build initial observation
        observation = self._build_observation()
        
        # Build info dict
        info = self._build_info()
        
        return observation, info
    
    def step(
        self,
        action: int,
    ) -> Tuple[np.ndarray, float, bool, bool, Dict[str, Any]]:
        """Execute one environment step.
        
        Args:
            action: Defensive action to take (0-4).
        
        Returns:
            Tuple of (observation, reward, terminated, truncated, info).
        """
        assert self.action_space.contains(action), f"Invalid action: {action}"
        
        self._step_count += 1
        
        # Store previous attack stage for reward calculation
        previous_attack_stage = self._current_attack_stage
        
        # Red Team: Advance attack sequence
        self._advance_attack()
        
        # Calculate reward based on defense effectiveness
        reward = self._calculate_reward(action, previous_attack_stage)
        
        # Update observation window
        current_stage = KillChainStage(self._current_attack_stage)
        new_features = self._realization_engine.sample(current_stage)
        self._observation_window.pop(0)
        self._observation_window.append(new_features)
        
        # Store action
        self._last_action = action
        
        # Check termination conditions
        terminated = self._check_terminated()
        truncated = self._step_count >= self._config.max_steps
        
        # Build observation and info
        observation = self._build_observation()
        info = self._build_info()
        
        return observation, reward, terminated, truncated, info
    
    def render(self) -> None:
        """Render the environment state."""
        if self._render_mode == "human":
            print(
                f"Step {self._step_count}: "
                f"Attack Stage = {KillChainStage(self._current_attack_stage).name}, "
                f"Last Action = {ACTION_NAMES[self._last_action]}"
            )
    
    def close(self) -> None:
        """Clean up environment resources."""
        pass
    
    # =========================================================================
    # Internal Methods
    # =========================================================================
    
    def _advance_attack(self) -> None:
        """Advance attack sequence using generator."""
        # Use generator to predict next attack stage
        if len(self._attack_history) >= 1:
            # Sample next stage from generator
            next_stage = self._generator.sample_next(
                self._attack_history[-5:],  # Use last 5 stages
                temperature=1.0,
            )
            self._current_attack_stage = next_stage
            self._attack_history.append(next_stage)
    
    def _build_observation(self) -> np.ndarray:
        """Build observation from feature window."""
        # Stack window and flatten
        window_array = np.stack(self._observation_window, axis=0)
        observation = window_array.flatten().astype(np.float32)
        return observation
    
    def _build_info(self) -> Dict[str, Any]:
        """Build info dictionary."""
        return {
            "attack_stage": self._current_attack_stage,
            "attack_stage_name": KillChainStage(self._current_attack_stage).name,
            "step_count": self._step_count,
            "attack_history": list(self._attack_history),
            "last_action": self._last_action,
            "last_action_name": ACTION_NAMES[self._last_action],
        }
    
    def _calculate_reward(self, action: int, previous_stage: int) -> float:
        """Calculate reward for the current step.
        
        Reward function: R_t = R_defense - C_action - P_impact
        
        Args:
            action: Defensive action taken.
            previous_stage: Attack stage before this step.
        
        Returns:
            Reward value.
        """
        reward = 0.0
        
        # C_action: Cost of defensive action
        action_cost = get_action_cost(action) * self._config.action_cost_scale
        reward -= action_cost
        
        # P_impact: Penalty if attack reached IMPACT
        if self._current_attack_stage == KillChainStage.IMPACT.value:
            reward -= self._config.impact_penalty
        
        # R_defense: Reward for appropriate defense
        attack_escalated = self._current_attack_stage > previous_stage
        attack_deescalated = self._current_attack_stage < previous_stage
        action_escalated = action > self._last_action
        action_deescalated = action < self._last_action
        
        # Reward matching defense to attack behavior
        if attack_escalated and action_escalated:
            # Correctly escalated defense
            reward += self._config.correct_escalation_reward
        elif attack_deescalated and action_deescalated:
            # Correctly de-escalated defense
            reward += self._config.correct_de_escalation_reward
        elif not attack_escalated and not attack_deescalated:
            # Attack persisted, maintained appropriate defense
            if self._is_appropriate_defense(action):
                reward += self._config.maintained_defense_reward
        
        # Bonus for keeping attack from reaching IMPACT
        if previous_stage == KillChainStage.MANEUVER.value:
            if self._current_attack_stage < KillChainStage.IMPACT.value:
                if action >= 3:  # BLOCK or ISOLATE
                    reward += self._config.defense_success_bonus
        
        return reward
    
    def _is_appropriate_defense(self, action: int) -> bool:
        """Check if defense level is appropriate for attack stage.
        
        Rough mapping:
        - BENIGN: OBSERVE is appropriate
        - RECON: LOG is appropriate  
        - ACCESS: THROTTLE is appropriate
        - MANEUVER: BLOCK is appropriate
        - IMPACT: ISOLATE is appropriate
        """
        # Map attack stage to recommended action
        recommended = min(self._current_attack_stage, 4)
        
        # Allow some flexibility (+/- 1)
        return abs(action - recommended) <= 1
    
    def _check_terminated(self) -> bool:
        """Check if episode should terminate.
        
        Terminates if:
        1. Attack reaches IMPACT stage (failure state) - immediate termination to prevent penalty accumulation.
        """
        if self._current_attack_stage == KillChainStage.IMPACT.value:
            return True
        
        return False
