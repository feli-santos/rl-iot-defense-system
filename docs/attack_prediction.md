# LSTM Attack Prediction Model

## Model Architecture

The attack prediction component uses a Long Short-Term Memory (LSTM) network to predict the next actions of an attacker based on observed attack patterns. This allows the defense system to anticipate attacks before they occur, enabling proactive defense.

### LSTM Network Structure

The LSTM model is implemented in `src/models/attack_predictor.py`. The architecture typically involves:
1.  **Embedding Layer**: Maps discrete input events (e.g., attack types, device IDs) to dense vector representations. The `num_embeddings` would correspond to the vocabulary size of unique events, and `embedding_dim` is specified by `config.LSTM_EMBEDDING_SIZE`.
2.  **Bidirectional LSTM Layers**: One or more LSTM layers process the sequence of embedded events. Bidirectionality allows the model to capture context from both past and future events in the sequence. The hidden sizes are defined by `config.LSTM_HIDDEN_UNITS` (e.g., `[h1_size, h2_size]`).
3.  **Dropout Layer**: Applied for regularization to prevent overfitting.
4.  **Fully Connected Layer**: Maps the output of the LSTM layers to the desired output space (e.g., probability distribution over possible next attack events, defined by `config.LSTM_OUTPUT_CLASSES`).

A representative implementation snippet from `src/models/attack_predictor.py`:
```python
# Simplified from LSTMAttackPredictor class
class LSTMAttackPredictor(nn.Module):
    def __init__(self, config, seq_length=10): # seq_length might be fixed or dynamic
        super(LSTMAttackPredictor, self).__init__()
        self.config = config
        self.seq_length = seq_length # Example: sequence length
        
        # Embedding layer (assuming num_unique_events is known)
        # num_unique_events should be based on your data_generator.py's event vocabulary
        num_unique_events = 227 # Example value, ensure this matches your data
        self.embedding = nn.Embedding(
            num_embeddings=num_unique_events, 
            embedding_dim=config.LSTM_EMBEDDING_SIZE
        )
        
        # Bidirectional LSTM layers
        # Ensure LSTM_HIDDEN_UNITS is defined in config.yml, e.g., [128, 64]
        self.lstm1 = nn.LSTM(
            input_size=config.LSTM_EMBEDDING_SIZE,
            hidden_size=config.LSTM_HIDDEN_UNITS[0], 
            bidirectional=True,
            batch_first=True
        )
        
        self.lstm2 = nn.LSTM(
            input_size=config.LSTM_HIDDEN_UNITS[0]*2,  # *2 for bidirectional
            hidden_size=config.LSTM_HIDDEN_UNITS[1],
            bidirectional=True,
            batch_first=True
        )
        
        self.dropout = nn.Dropout(0.5) # Dropout rate can be configured
        
        # Fully connected output layer
        # Ensure LSTM_OUTPUT_CLASSES is defined in config.yml (e.g., num_unique_events for next event prediction)
        self.fc = nn.Linear(
            in_features=config.LSTM_HIDDEN_UNITS[1]*2, # *2 for bidirectional
            out_features=config.LSTM_OUTPUT_CLASSES 
        )
```
*Note: `config.LSTM_HIDDEN_UNITS` and `config.LSTM_OUTPUT_CLASSES` should be defined in your `config.yml` for this structure to work as described.*

## Mathematical Foundation

### LSTM Cell

Each LSTM cell processes sequence elements with the following operations:

1. **Forget Gate** - Determines what information to discard from cell state:
   $$f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$$

2. **Input Gate** - Determines what new information to store in cell state:
   $$i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$$
   $$\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)$$

3. **Cell State Update** - Updates the cell state:
   $$C_t = f_t * C_{t-1} + i_t * \tilde{C}_t$$

4. **Output Gate** - Determines what to output based on cell state:
   $$o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$$
   $$h_t = o_t * \tanh(C_t)$$

Where:
- $\sigma$ is the sigmoid function
- $[h_{t-1}, x_t]$ is the concatenation of the previous hidden state and current input
- $W$ and $b$ are weight matrices and bias vectors
- $*$ denotes element-wise multiplication

### Bidirectional Architecture

The bidirectional LSTM processes the sequence in both forward and backward directions:

$$\overrightarrow{h}_t = \text{LSTM}_{\text{forward}}(x_t, \overrightarrow{h}_{t-1})$$
$$\overleftarrow{h}_t = \text{LSTM}_{\text{backward}}(x_t, \overleftarrow{h}_{t+1})$$
$$h_t = [\overrightarrow{h}_t, \overleftarrow{h}_t]$$

This allows the model to capture dependencies from both past and future contexts in the sequence.

## Training Process

### Data Generation

Training data, consisting of sequences of attack events and corresponding labels (e.g., the next event in the sequence), is generated by the `RealisticAttackDataGenerator` class in `src/utils/data_generator.py`. This generator aims to create plausible attack scenarios.

```python
# Conceptual representation from RealisticAttackDataGenerator
# The actual implementation will depend on how sequences and labels are formed.
# For predicting the next event, X would be a sequence, and y the next event.
class RealisticAttackDataGenerator:
    def __init__(self, num_states, num_sequences, seq_length, attack_patterns):
        # ... initialization ...
        pass

    def generate_data(self):
        # Generates X: (num_sequences, seq_length) and y: (num_sequences,)
        # or X: (num_sequences, seq_length, feature_dim) for feature-rich events
        # ... implementation ...
        sequences = [] # List of sequences
        labels = []    # List of corresponding next events/labels
        # ... logic to generate realistic sequences ...
        return np.array(sequences), np.array(labels)
```

### Training Loop
The LSTM model is trained within the `train_lstm_attack_predictor` function in `src/training.py`.
Key aspects of the training loop include:
1.  **Optimizer**: Typically Adam, with learning rate and weight decay configurable.
2.  **Loss Function**: Cross-Entropy Loss is suitable for predicting the class of the next attack event.
3.  **Gradient Clipping**: Applied using `torch.nn.utils.clip_grad_norm_` with `config.LSTM_GRADIENT_CLIP_NORM` to prevent exploding gradients.
4.  **Data Loading**: Uses PyTorch `DataLoader` for batching and shuffling.
5.  **Train-Validation Split**: Data is split to monitor for overfitting and save the best model based on validation performance.
6.  **Metrics Tracking**: Training and validation loss/accuracy are logged per epoch using the `TrainingManager`.

```python
# Simplified conceptual training loop from src/models/attack_predictor.py train_model method
# and src/training.py train_lstm_attack_predictor function

# In LSTMAttackPredictor class:
# self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate, weight_decay=weight_decay)
# self.criterion = nn.CrossEntropyLoss()

# In train_lstm_attack_predictor:
# lstm_model.train_model(epochs=100, batch_size=32) # Calls the method in LSTMAttackPredictor

# Inside LSTMAttackPredictor.train_model:
for epoch in range(epochs):
    # Training phase
    model.train()
    for inputs, targets in train_loader: # inputs: (batch, seq_len), targets: (batch,)
        optimizer.zero_grad()
        outputs = model(inputs) # outputs: (batch, num_classes)
        loss = criterion(outputs, targets)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), config.LSTM_GRADIENT_CLIP_NORM)
        optimizer.step()
        # ... track metrics ...
    
    # Validation phase
    model.eval()
    with torch.no_grad():
        for inputs, targets in val_loader:
            outputs = model(inputs)
            # ... track metrics ...
    # ... log epoch metrics via TrainingManager ...
```

## Inference Process

During operation, the trained LSTM model takes a sequence of recent events as input and predicts the probability distribution over possible next events.
```python
# Conceptual inference from LSTMAttackPredictor
def predict_next_event(self, event_sequence_tensor): # event_sequence_tensor: (1, seq_length)
    self.eval()
    with torch.no_grad():
        output_logits = self(event_sequence_tensor) # output_logits: (1, num_classes)
        probabilities = torch.softmax(output_logits, dim=1)
        predicted_event_index = torch.argmax(probabilities, dim=1).item()
    return predicted_event_index, probabilities
```

## Performance Metrics

The LSTM model's performance is evaluated using:
1.  **Validation Accuracy**: Percentage of correctly predicted next attack steps on the validation set.
2.  **Validation Loss**: Cross-entropy loss on the validation set.
These metrics are logged by the `TrainingManager` and help in selecting the best model checkpoint.

Example performance (from `training.py` output):
- Best validation loss: (e.g., 0.0635)
- Best validation accuracy: (e.g., 0.9590)