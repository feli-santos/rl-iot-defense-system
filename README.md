# IoT Defense System - Adversarial Reinforcement Learning

A research project using **Adversarial Reinforcement Learning** for IoT network defense. The system implements a Red Team vs Blue Team paradigm where an **LSTM-based Attack Sequence Generator** (Red Team) learns realistic attack patterns from the CICIoT2023 dataset, and **RL Defense Agents** (Blue Team) learn optimal defense policies through adversarial training.

## Key Features

- **Kill Chain Abstraction**: 33 CICIoT2023 attack classes mapped to 5 tactical stages (BENIGN → RECON → ACCESS → MANEUVER → IMPACT)
- **Attack Sequence Generator**: LSTM next-token predictor trained on real attack patterns
- **Adversarial Environment**: Gymnasium-compatible environment with hidden attack state and force continuum actions
- **Multiple RL Algorithms**: DQN, PPO, and A2C via Stable Baselines3
- **MLflow Integration**: Experiment tracking and model versioning
- **Comprehensive Testing**: 179+ unit tests with pytest

## Architecture

```
┌─────────────────────────────────────────────────────────────────────┐
│                    ADVERSARIAL TRAINING LOOP                        │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  ┌─────────────────────┐         ┌─────────────────────────────┐   │
│  │    RED TEAM         │         │        BLUE TEAM            │   │
│  │    (Attacker)       │         │        (Defender)           │   │
│  ├─────────────────────┤         ├─────────────────────────────┤   │
│  │                     │         │                             │   │
│  │  Attack Sequence    │  attack │    RL Defense Agent         │   │
│  │  Generator (LSTM)   │ ──────► │    (DQN/PPO/A2C)            │   │
│  │                     │  state  │                             │   │
│  │  Next-token         │         │    Force Continuum:         │   │
│  │  predictor for      │         │    • MONITOR (observe)      │   │
│  │  Kill Chain stages  │ ◄────── │    • RATE_LIMIT (slow)      │   │
│  │                     │ defense │    • BLOCK (deny)           │   │
│  │                     │ action  │    • ISOLATE (contain)      │   │
│  └─────────────────────┘         └─────────────────────────────┘   │
│            │                                   │                    │
│            ▼                                   ▼                    │
│  ┌─────────────────────┐         ┌─────────────────────────────┐   │
│  │  CICIoT2023 Dataset │         │   AdversarialIoTEnv         │   │
│  │  (33 attack classes)│         │   (Gymnasium Environment)   │   │
│  └─────────────────────┘         └─────────────────────────────┘   │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
```

## Kill Chain Stages

| Stage | ID | Description | Example Attacks |
|-------|----|----|-----------------|
| BENIGN | 0 | Normal network traffic | Legitimate IoT communication |
| RECON | 1 | Reconnaissance | Port scanning, vulnerability probing |
| ACCESS | 2 | Initial access | Brute force, credential stuffing |
| MANEUVER | 3 | Lateral movement | Backdoor, C2 communication |
| IMPACT | 4 | Final objective | DDoS, ransomware, data theft |

## Installation

### Prerequisites

- Python 3.12+
- Virtual environment recommended

### Setup

```bash
# Clone repository
git clone https://github.com/your-username/rl-iot-defense-system.git
cd rl-iot-defense-system

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Dataset

Download the CICIoT2023 dataset and place it in `data/raw/CICIoT2023/`:

```bash
# Directory structure after download
data/
├── raw/
│   └── CICIoT2023/
│       └── *.csv  # Attack trace files
└── processed/
    └── ciciot2023/
        └── *.parquet  # Processed files (generated by pipeline)
```

## Usage

### Command Line Interface

The system provides a unified CLI via `main.py`:

```bash
# View all available commands
python main.py --help

# View command-specific help
python main.py train-generator --help
```

### Pipeline Modes

#### 1. Process Dataset

Convert raw CICIoT2023 CSV files to processed format with Kill Chain labels:

```bash
python main.py process-data \
    --input-dir data/raw/CICIoT2023 \
    --output-dir data/processed/ciciot2023
```

#### 2. Train Attack Sequence Generator (Red Team)

Train the LSTM-based attack sequence generator:

```bash
python main.py train-generator \
    --data-dir data/processed/ciciot2023 \
    --epochs 50 \
    --batch-size 64 \
    --seq-length 16 \
    --hidden-dim 128
```

#### 3. Train Defense Agent (Blue Team)

Train an RL defense agent against the attack generator:

```bash
# Train with PPO (recommended)
python main.py train-rl \
    --algorithm ppo \
    --total-timesteps 100000 \
    --generator-model artifacts/generator/attack_generator.pth

# Train with DQN
python main.py train-rl \
    --algorithm dqn \
    --total-timesteps 100000

# Train with A2C
python main.py train-rl \
    --algorithm a2c \
    --total-timesteps 100000
```

#### 4. Full Training Pipeline

Run the complete training pipeline (generator + RL):

```bash
python main.py train-all \
    --data-dir data/processed/ciciot2023 \
    --algorithm ppo \
    --generator-epochs 50 \
    --rl-timesteps 100000
```

#### 5. Evaluate Trained Agent

Evaluate a trained defense agent:

```bash
python main.py evaluate \
    --model-path artifacts/rl/ppo_model.zip \
    --episodes 100
```

#### 6. Benchmark All Algorithms

Compare DQN, PPO, and A2C performance:

```bash
python main.py benchmark \
    --timesteps 50000 \
    --eval-episodes 100
```

## Configuration

All parameters can be configured via `config.yml`:

```yaml
# Attack Sequence Generator
attack_generator:
  embed_dim: 64
  hidden_dim: 128
  num_layers: 2
  dropout: 0.2
  learning_rate: 0.001

# Episode Generation
episode_generation:
  min_episode_length: 5
  max_episode_length: 50
  attack_probability: 0.7

# Adversarial Environment
adversarial_env:
  max_steps: 100
  window_size: 10
  observation_dim: 64
  defense_costs:
    monitor: 0.0
    rate_limit: 0.1
    block: 0.3
    isolate: 0.5
  impact_penalties:
    recon: 0.1
    access: 0.3
    maneuver: 0.5
    impact: 1.0

# RL Algorithms
rl:
  algorithm: ppo  # dqn, ppo, a2c
  total_timesteps: 100000
  learning_rate: 0.0003
  
  dqn:
    buffer_size: 100000
    learning_starts: 1000
    batch_size: 64
    
  ppo:
    n_steps: 2048
    batch_size: 64
    n_epochs: 10
    
  a2c:
    n_steps: 5
    ent_coef: 0.01
```

## Project Structure

```
rl-iot-defense-system/
├── main.py                          # Unified CLI entry point
├── config.yml                       # Configuration file
├── requirements.txt                 # Python dependencies
│
├── src/
│   ├── algorithms/
│   │   ├── __init__.py
│   │   └── adversarial_algorithm.py # RL algorithm wrapper (DQN/PPO/A2C)
│   │
│   ├── environment/
│   │   ├── __init__.py
│   │   └── adversarial_env.py       # Gymnasium environment
│   │
│   ├── generator/
│   │   ├── __init__.py
│   │   ├── attack_sequence_generator.py  # LSTM Red Team model
│   │   └── episode_generator.py          # Episode synthesis
│   │
│   ├── training/
│   │   ├── __init__.py
│   │   ├── generator_trainer.py     # Red Team training loop
│   │   └── training_manager.py      # MLflow integration
│   │
│   └── utils/
│       ├── __init__.py
│       ├── config_loader.py         # Configuration loading
│       ├── dataset_loader.py        # Data loading utilities
│       ├── dataset_processor.py     # CICIoT2023 processing
│       ├── label_mapper.py          # Attack → Kill Chain mapping
│       └── realization_engine.py    # Feature vector sampling
│
├── tests/
│   ├── test_adversarial_algorithm.py
│   ├── test_adversarial_env.py
│   ├── test_attack_sequence_generator.py
│   ├── test_episode_generator.py
│   ├── test_generator_trainer.py
│   ├── test_label_mapper.py
│   └── test_realization_engine.py
│
├── data/
│   ├── raw/CICIoT2023/              # Raw dataset
│   └── processed/ciciot2023/        # Processed data
│
├── artifacts/
│   ├── generator/                   # Trained generator models
│   └── rl/                          # Trained RL models
│
├── mlruns/                          # MLflow experiment tracking
│
├── results/
│   ├── benchmark/                   # Benchmark results
│   └── plots/                       # Training visualizations
│
└── docs/
    ├── PRD.md                       # Product Requirements Document
    ├── overview.md                  # System overview
    └── *.md                         # Additional documentation
```

## Reward Function

The defense agent's reward is calculated as:

$$R_t = R_{defense} - C_{action} - P_{impact}$$

Where:
- $R_{defense}$: Reward for successful defense (blocking attack progression)
- $C_{action}$: Cost of the defense action taken (force continuum)
- $P_{impact}$: Penalty if attack reaches IMPACT stage

## Testing

Run the test suite:

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src

# Run specific test file
pytest tests/test_adversarial_env.py -v

# Run tests matching pattern
pytest -k "test_generator" -v
```

## MLflow Tracking

View experiment results:

```bash
# Start MLflow UI
mlflow ui

# Open in browser: http://localhost:5000
```

## Research Background

This project is based on the CICIoT2023 dataset for IoT security research. The adversarial training paradigm allows RL agents to learn robust defense policies against realistic attack sequences.

### References

- CICIoT2023 Dataset: Canadian Institute for Cybersecurity
- Stable Baselines3: [https://stable-baselines3.readthedocs.io/](https://stable-baselines3.readthedocs.io/)
- Gymnasium: [https://gymnasium.farama.org/](https://gymnasium.farama.org/)

## License

See [LICENSE](LICENSE) for details.

## Contributing

1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Ensure all tests pass
5. Submit a pull request
