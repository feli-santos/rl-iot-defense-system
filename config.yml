# IoT Defense System Configuration
# Main configuration for Adversarial RL Defense System
#
# System Overview (PRD):
# - Red Team: Attack Sequence Generator (LSTM) produces attack sequences
# - Blue Team: RL agents (DQN/PPO/A2C) learn defense policies
# - Kill Chain Stages: BENIGN(0), RECON(1), ACCESS(2), MANEUVER(3), IMPACT(4)

# Dataset Configuration
dataset:
  name: "CICIoT2023"
  raw_path: "data/raw/CICIoT2023"
  processed_path: "data/processed/ciciot2023"
  sample_size: 500_000 # Start with 500K samples for manageable training
  sequence_length: 5 # EDA recommendation
  train_split: 0.7
  val_split: 0.1
  test_split: 0.2
  # Add class balancing strategy
  sampling_strategy: "balanced" # For severe imbalance
  feature_selection: true # Remove low/zero variance features

# =============================================================================
# ATTACK SEQUENCE GENERATOR (RED TEAM)
# =============================================================================

# Episode Generation Configuration
# Generates synthetic attack episodes following Kill Chain grammar
episode_generation:
  num_episodes: 10000 # Number of training episodes
  min_length: 5 # Minimum episode length
  max_length: 30 # Maximum episode length
  num_stages: 5 # Kill Chain stages (fixed)
  benign_start_prob: 0.8 # Probability of starting with BENIGN
  progression_weight: 0.5 # Weight for escalation transitions
  persistence_weight: 0.3 # Weight for staying at same stage
  skip_weight: 0.2 # Weight for skipping stages

# Attack Sequence Generator (LSTM) Configuration
attack_generator:
  model:
    num_stages: 5 # Kill Chain stages (fixed)
    embedding_dim: 32 # Stage embedding dimension
    hidden_size: 64 # LSTM hidden state size
    num_layers: 2 # Number of LSTM layers
    dropout: 0.1 # Dropout between LSTM layers
    temperature: 1.0 # Default sampling temperature

  training:
    epochs: 50 # Training epochs
    batch_size: 32 # Training batch size
    learning_rate: 0.001 # Adam learning rate
    sequence_length: 5 # Input sequence length
    val_split: 0.2 # Validation split
    early_stopping_patience: 10 # Epochs before early stopping
    device: "cpu" # Training device (cpu, cuda, mps)

  output:
    save_dir: "artifacts/generator" # Model output directory

# =============================================================================
# ADVERSARIAL ENVIRONMENT (BLUE TEAM TRAINING)
# =============================================================================

# Adversarial Environment Configuration
# Agent observes realized feature vectors, hidden attack state is not visible
adversarial_environment:
  max_steps: 500 # Steps per episode
  observation:
    # Box observation space: window of realized features
    window_size: 5 # Number of previous observations in window
    # Feature dimension determined at runtime from dataset

  # Force Continuum Actions (5 discrete levels)
  # Higher levels = more defensive but more costly
  actions:
    num_actions: 5
    levels:
      - name: "OBSERVE"
        description: "Passive monitoring, no intervention"
        cost: 0.0
      - name: "LOG"
        description: "Enhanced logging, minimal impact"
        cost: 0.1
      - name: "THROTTLE"
        description: "Rate limiting suspicious traffic"
        cost: 0.3
      - name: "BLOCK"
        description: "Block suspicious IPs/connections"
        cost: 0.5
      - name: "ISOLATE"
        description: "Isolate affected systems"
        cost: 0.8

  # Reward Function: R_t = R_defense - C_action - P_impact
  reward:
    # R_defense: positive reward for correct defense actions
    defense_reward:
      correct_escalation: 1.0 # Escalate when attack escalates
      correct_de_escalation: 0.5 # De-escalate when threat reduces
      maintained_defense: 0.2 # Hold appropriate level

    # C_action: cost of defensive actions (from actions.levels.cost)
    action_cost_scale: 1.0

    # P_impact: penalty when attack reaches IMPACT stage
    impact_penalty: 5.0

    # Bonus for successfully defending (attack terminates at lower stage)
    defense_success_bonus: 2.0

# RL Training Configuration
rl:
  algorithm: "dqn" # Options: dqn, ppo, a2c

  training:
    total_timesteps: 50000 # Reduced for initial training
    eval_freq: 5000
    n_eval_episodes: 10
    save_freq: 12500

  # Algorithm-specific hyperparameters
  algorithms:
    dqn:
      learning_rate: 1e-4
      buffer_size: 50000 # Adjusted for sample size
      learning_starts: 1000
      batch_size: 32
      tau: 1.0
      gamma: 0.99
      train_freq: 4
      gradient_steps: 1
      target_update_interval: 1000
      exploration_fraction: 0.1
      exploration_initial_eps: 1.0
      exploration_final_eps: 0.05
      max_grad_norm: 10

    ppo:
      learning_rate: 3e-4
      n_steps: 2048
      batch_size: 64
      n_epochs: 10
      gamma: 0.99
      gae_lambda: 0.95
      clip_range: 0.2
      clip_range_vf: null
      normalize_advantage: true
      ent_coef: 0.0
      vf_coef: 0.5
      max_grad_norm: 0.5

    a2c:
      learning_rate: 7e-4
      n_steps: 5
      gamma: 0.99
      gae_lambda: 1.0
      ent_coef: 0.0
      vf_coef: 0.5
      max_grad_norm: 0.5
      rms_prop_eps: 1e-5
      use_rms_prop: true
      use_sde: false

# Model Paths
models:
  # Attack Sequence Generator (Red Team)
  generator:
    save_dir: "artifacts/generator"
    model_file: "attack_sequence_generator.pth"
    config_file: "config.json"
    training_config_file: "training_config.json"

  # RL Agent (Blue Team)
  rl:
    save_dir: "artifacts/rl"
    checkpoint_dir: "artifacts/rl/checkpoints"

# Experiment Tracking
mlflow:
  tracking_uri: "mlruns"
  experiment_name: "iot_defense_system"

# Logging Configuration
logging:
  level: "INFO"
  log_dir: "results/logs"

# Results Configuration
results:
  plots_dir: "results/plots"
  reports_dir: "results/reports"
