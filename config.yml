# Configuration File

# Environment parameters
environment:
  num_devices: 12
  num_actions: 4
  num_states: 12
  history_length: 5

# Algorithm selection
algorithm:
  type: "DQN"
  algorithms_to_compare: ["DQN", "PPO", "A2C"] # Options

# DQN hyperparameters
dqn:
  total_episodes: 1000
  epochs_per_episode: 25 # Reduced to match total timesteps with PPO/A2C
  learning_rate: 0.001
  gamma: 0.99
  batch_size: 64
  buffer_size: 10000
  tau: 0.005 # for target network update
  target_update_freq: 10 # episodes

# PPO hyperparameters
ppo:
  total_timesteps: 25000
  learning_rate: 0.0003
  n_steps: 2048
  batch_size: 64
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  ent_coef: 0.0
  vf_coef: 0.5
  max_grad_norm: 0.5

# A2C hyperparameters
a2c:
  total_timesteps: 25000
  learning_rate: 0.0007
  n_steps: 5
  gamma: 0.99
  gae_lambda: 0.95
  ent_coef: 0.01
  vf_coef: 0.25
  max_grad_norm: 0.5

# Exploration parameters
exploration:
  eps_start: 1.0
  eps_end: 0.01
  eps_decay: 0.995

# Reward function parameters
reward:
  injection_threshold: 0.25 # k value
  goal_reward: -100 # Gr value

# Neural network architecture
network:
  hidden_layers: [128, 64]

# LSTM model parameters
lstm:
  embedding_size: 128
  hidden_units: [64, 32]
  output_classes: 12
  gradient_clip_norm: 1.0

# Attack model parameters
attack:
  adaptation_rate: 0.2
  target_probability: 0.3

# Training configuration
training:
  device: "auto"
  verbose: 1
  seed: 42
  model_dir: "./models/"
  tensorboard_dir: "./tensorboard/"
  logs_dir: "./logs/"

# Benchmarking configuration
benchmarking:
  num_runs: 3 # Number of independent runs per algorithm
  evaluation_episodes: 1000
  metrics_to_track:
    ["avg_reward", "episode_length", "success_rate", "convergence_time"]
  save_plots: true
  comparison_plots: true
